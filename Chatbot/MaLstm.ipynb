{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaLstm.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMfpI8AKZnFqwJd4wgwoUGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeinabmohamed/CIT690E-DeepLearning-Zeinab_Abdelmawla-191009/blob/main/Chatbot/MaLstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n"
      ],
      "metadata": {
        "id": "TGAEhHCMhmBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8-3mpG6KLNO"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "!pip install --user -U nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda\n",
        "import keras.backend as K\n",
        "# from keras.optimizers import Adadelta\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download quora-question-pairs -f data?select=test.csv.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "9go-xpwGLwaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eRdC0FnDNYAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download quora-question-pairs -f test.csv.zip\n",
        "! kaggle competitions download quora-question-pairs -f train.csv.zip\n",
        "\n",
        "! unzip -o test.csv.zip\n",
        "! unzip -o train.csv.zip"
      ],
      "metadata": {
        "id": "YVq85AAwM_Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "url = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "filename = wget.download(url)"
      ],
      "metadata": {
        "id": "Hx0kYp0KR-rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "#https://www.kaggle.com/c/quora-question-pairs/data?select=test.csv.zip\n",
        "TRAIN_CSV = './train.csv'\n",
        "TEST_CSV = './test.csv'\n",
        "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin.gz'\n",
        "MODEL_SAVING_DIR = '/home/ecohen/HDD/HDD4/Models/Kaggle/Quora/'"
      ],
      "metadata": {
        "id": "JNvjyvxPKt1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)"
      ],
      "metadata": {
        "id": "-g_Nnt_fN1-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_df)"
      ],
      "metadata": {
        "id": "qKvhHo38YF0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "fmedzmMaOphM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "j4ROEmARVUPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "TYO0CXInPz9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stops = set(stopwords.words('english'))\n",
        "stops"
      ],
      "metadata": {
        "id": "vEuVooQPO0_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "pPRQMAmvQTHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Example \n",
        "  sample = \"I am Zeinab\"\n",
        "  sample = re.sub(r\"Zeinab\", \"i\", sample)\n",
        "  sample"
      ],
      "metadata": {
        "id": "h0OydCN9V7tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example \n",
        "text_to_word_list(\"How do I out get rid of n't Erectile Dysfunction?\")"
      ],
      "metadata": {
        "id": "0vEpZ6naVo6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example\n",
        "#word2vec.vocab['egypt']\n",
        "#'egypt' not in word2vec.vocab\n",
        "#word2vec.vocab['of']\n",
        "'of' not in word2vec.vocab"
      ],
      "metadata": {
        "id": "1Cx1NVatZnYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare embedding\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "metadata": {
        "id": "6pjI_0-LQfxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example\n",
        "len(word2vec['egypt'])"
      ],
      "metadata": {
        "id": "ZNjcwmlki_xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding"
      ],
      "metadata": {
        "id": "xHeg_elVgcdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame()\n",
        "# Iterate through the text of both questions of the row\n",
        "#for question in questions_cols:\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']\n",
        "q2n = []  # q2n -> question numbers representation\n",
        "for word in text_to_word_list(\"How zeinab shouldn do I out get rid of n't zeinab Erectile Dysfunction?\"):\n",
        "  # Check for unwanted words\n",
        "  if word in stops and word not in word2vec.vocab:\n",
        "      print('Stop word : ',word)\n",
        "      continue\n",
        "\n",
        "  if word not in vocabulary:\n",
        "      print('Other word : ',word)\n",
        "      vocabulary[word] = len(inverse_vocabulary)\n",
        "      q2n.append(len(inverse_vocabulary))\n",
        "      inverse_vocabulary.append(word)\n",
        "  else:\n",
        "      print('else word : ',word)\n",
        "      q2n.append(vocabulary[word])\n",
        "\n",
        "    # Replace questions as word to question as number representation\n",
        "    # print(0)\n",
        "    # print(question)\n",
        "    # print(q2n)\n",
        "print(vocabulary)\n",
        "print(q2n)"
      ],
      "metadata": {
        "id": "uztfXWz0XgYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "word2vec['egypt']\n",
        "similarity = word2vec.similarity('egypt', 'car')\n",
        "similarity"
      ],
      "metadata": {
        "id": "kGEy4DX0Tbx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "questions_cols = ['question1', 'question2']\n",
        "# Iterate over the questions only of both training and test datasets\n",
        "for dataset in tqdm([train_df, test_df]):\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in questions_cols:\n",
        "\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "\n",
        "                # Check for unwanted words\n",
        "                if word in stops and word not in word2vec.vocab:\n",
        "                    continue\n",
        "\n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(inverse_vocabulary)\n",
        "                    q2n.append(len(inverse_vocabulary))\n",
        "                    inverse_vocabulary.append(word)\n",
        "                else:\n",
        "                    q2n.append(vocabulary[word])\n",
        "            # Replace questions as word to question as number representation\n",
        "            dataset.at[index, question]=q2n            "
      ],
      "metadata": {
        "id": "0JGDq1v6UDYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "pUdxZsJx2IcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embeddings[index] = word2vec.word_vec(word)\n",
        "\n",
        "del word2vec"
      ],
      "metadata": {
        "id": "UM36UGHpjtF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "TtvCdmij2SXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max(),\n",
        "                     test_df.question1.map(lambda x: len(x)).max(),\n",
        "                     test_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "max_seq_length"
      ],
      "metadata": {
        "id": "9iUc9hf-5nvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')"
      ],
      "metadata": {
        "id": "CWTa7OcI7DMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)"
      ],
      "metadata": {
        "id": "lN6QgiHW5YbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)"
      ],
      "metadata": {
        "id": "1G8pLaUm7MIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " embedding_layer.build(())"
      ],
      "metadata": {
        "id": "TueQGccu_Z94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weights"
      ],
      "metadata": {
        "id": "nx_QusLJ_fHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vec[word2vec.index2word[6]]"
      ],
      "metadata": {
        "id": "IVJ0M6JvUOQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1owaN3L2UOHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}