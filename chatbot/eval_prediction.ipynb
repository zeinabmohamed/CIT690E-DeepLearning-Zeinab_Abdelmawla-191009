{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval_prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "103IcJjInlbmAyJidoshN2m8cU9cQi1II",
      "authorship_tag": "ABX9TyPSnezaxJsHSDPW0QRO4Ajn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeinabmohamed/CIT690E-DeepLearning-Zeinab_Abdelmawla-191009/blob/main/chatbot/eval_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def readJson(filename):\n",
        "  with open(filename, 'r') as file:\n",
        "    intents = json.load(file)\n",
        "    return intents"
      ],
      "metadata": {
        "id": "MvJxrd39x59v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questionDoctorQAs = readJson('questionDoctorQAs.json')"
      ],
      "metadata": {
        "id": "lR3kFRsix2D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "all_medical_df = pd.DataFrame.from_dict(questionDoctorQAs)\n",
        "all_medical_df"
      ],
      "metadata": {
        "id": "plUEk78cyg0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "!pip install wget\n",
        "import wget\n",
        "\n",
        "url = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "filename = wget.download(url)\n",
        "\n",
        "embedding_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "embedding_dict_lib = KeyedVectors.load_word2vec_format(embedding_path, binary=True)"
      ],
      "metadata": {
        "id": "TgaxLwNH1fXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('./en_ameseLSTM.h5', custom_objects={'ManDist': ManDist})\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yVg4ELPY6TuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_col = all_medical_df[\"question\"]\n",
        "final_eval_result = pd.DataFrame(columns=['InputQ','RealatedQ','RealatedQ_prediaction','RealatedQ_sucess_predication','RealatedQ_index','RealatedQ_question_text','RealatedQ_answer','RealatedQ_answer_author','RealatedQ_tags','RealatedQ_url'])\n",
        "#range(len(question_col))\n",
        "for curentSelectedQIndex in range(20):\n",
        "  # preaper test DF\n",
        "  test_df = pd.DataFrame(question_col)\n",
        "  test_df.columns = ['question1']\n",
        "  test_df['question2']=test_df['question1'][curentSelectedQIndex]\n",
        "  ##################\n",
        "\n",
        "  predictions = predirct(test_df,embedding_dict_lib)\n",
        "\n",
        "  Eval = pd.DataFrame(columns= ['question1','question2','prediction'])\n",
        "  Eval['question1']= test_df['question1']\n",
        "  Eval['question2']=test_df['question2']\n",
        "  Eval['prediction']= predictions\n",
        "  \n",
        "  ######### start predicat related Question ############\n",
        "\n",
        "  validateEvalDirExist('./Eval/')\n",
        "  evalFileName = './Eval/Eval_question_'+str(curentSelectedQIndex)+'.csv'\n",
        "  Eval.to_csv(evalFileName)\n",
        "  column = Eval[\"prediction\"]\n",
        "  predictionـcol = Eval[\"prediction\"]\n",
        "  max_index = predictionـcol.idxmax()\n",
        "  max_predited_quetion_row = Eval.iloc[[max_index]]\n",
        "  prediacted_related_question_row = all_medical_df.iloc[[max_index]]\n",
        "  print(\"Question .. \",curentSelectedQIndex,\" : \",max_predited_quetion_row.iloc[0]['question2'])\n",
        "\n",
        "  ######### save predicated Question ############\n",
        "  isSucess = 1 if prediacted_related_question_row.iloc[0]['question']== max_predited_quetion_row.iloc[0]['question2'] else 0\n",
        "  new_row = {'InputQ':max_predited_quetion_row.iloc[0]['question2'],\n",
        "             'RealatedQ':prediacted_related_question_row.iloc[0]['question'],\n",
        "             'RealatedQ_prediaction':max_predited_quetion_row.iloc[0]['prediction'],\n",
        "             'RealatedQ_sucess_predication':isSucess,\n",
        "             'RealatedQ_index':max_index,\n",
        "             'RealatedQ_question_text':prediacted_related_question_row.iloc[0]['question_text'],\n",
        "             'RealatedQ_answer':prediacted_related_question_row.iloc[0]['answer'],\n",
        "             'RealatedQ_answer_author':prediacted_related_question_row.iloc[0]['answer_author'],\n",
        "             'RealatedQ_tags':prediacted_related_question_row.iloc[0]['tags'],\n",
        "             'RealatedQ_url':prediacted_related_question_row.iloc[0]['url']\n",
        "             }\n",
        "  final_eval_result = final_eval_result.append(new_row,ignore_index=True)\n",
        "\n",
        "final_eval_result.head(10)\n",
        "\n",
        "preicatedSucessRate = (final_eval_result['RealatedQ_sucess_predication'].sum()/len(final_eval_result))*100\n",
        "print(\"preicatedSucessRate\",preicatedSucessRate)\n",
        "######### Export final result to csv ######### \n",
        "finalEvalFileName = './Eval/Final_Eval_question.csv'\n",
        "final_eval_result.to_csv(finalEvalFileName)"
      ],
      "metadata": {
        "id": "-Ccd--OJxxCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functaions:"
      ],
      "metadata": {
        "id": "8osCSCIKuW2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def predirct(input_test_df,embedding_dict):\n",
        "  # Load training set\n",
        "  for q in ['question1', 'question2']:\n",
        "    input_test_df[q + '_n'] = input_test_df[q]\n",
        "\n",
        "  # Make word2vec embeddings\n",
        "  embedding_dim = 300\n",
        "  max_seq_length = 32\n",
        "  \n",
        "  test_df, embeddings = make_w2v_embeddings(embedding_dict, input_test_df, embedding_dim=embedding_dim)\n",
        "\n",
        "  # Split to dicts and append zero padding.\n",
        "  X_test = split_and_zero_padding(test_df, max_seq_length)\n",
        "\n",
        "  # Make sure everything is ok\n",
        "  assert X_test['left'].shape == X_test['right'].shape\n",
        "\n",
        "  # --\n",
        "\n",
        "  prediction = model.predict([X_test['left'], X_test['right']])\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "zZ_5Cgi6xvlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_word_list(text):  # 文本分词\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    # Text cleaning rules for English text\n",
        "    import re\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "o_cTdRzyuxCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def make_w2v_embeddings(word2vec, df, embedding_dim):  # 将词转化为词向量\n",
        "    vocabs = {}  # 词序号\n",
        "    vocabs_count = 0  # 词个数计数器\n",
        "\n",
        "    vocabs_not_w2v = {}  # 无法用词向量表示的词\n",
        "    vocabs_not_w2v_count = 0  # Word count that cannot be represented by word vectors\n",
        "\n",
        "    # 停用词\n",
        "    # stops = set(open('data/stopwords.txt').read().strip().split('\\n'))\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # 打印处理进度\n",
        "        if index != 0 and index % 1000 == 0:\n",
        "            print(str(index) + \" sentences embedded.\")\n",
        "\n",
        "        for question in ['question1', 'question2']:\n",
        "            q2n = []  # q2n -> question to numbers representation\n",
        "            words = text_to_word_list(row[question])\n",
        "\n",
        "            for word in words:\n",
        "                # if word in stops:  # remove stop words\n",
        "                # continue\n",
        "                # The word of OOV is put into a dictionary that cannot be represented by a word vector, and the value is 1\n",
        "                if word not in word2vec and word not in vocabs_not_w2v:  \n",
        "                    vocabs_not_w2v_count += 1\n",
        "                    vocabs_not_w2v[word] = 1   \n",
        "                if word not in vocabs:  # Non-OOV words, extract the corresponding id\n",
        "                    vocabs_count += 1\n",
        "                    vocabs[word] = vocabs_count\n",
        "                    q2n.append(vocabs_count)\n",
        "                else:\n",
        "                    q2n.append(vocabs[word])\n",
        "            df.at[index, question + '_n'] = q2n\n",
        "\n",
        "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # 随机初始化一个形状为[全部词个数，词向量维度]的矩阵\n",
        "    '''\n",
        "    词1 [a1, a2, a3, ..., a60]\n",
        "    词2 [b1, b2, b3, ..., b60]\n",
        "    词3 [c1, c2, c3, ..., c60]\n",
        "    '''\n",
        "    embeddings[0] = 0  # 第一行用0填充，因为不存在index为0的词\n",
        "\n",
        "    for index in vocabs:\n",
        "        vocab_word = vocabs[index]\n",
        "        if vocab_word in word2vec:\n",
        "            embeddings[index] = word2vec[vocab_word]\n",
        "    del word2vec\n",
        "\n",
        "    return df, embeddings"
      ],
      "metadata": {
        "id": "eFijDDv7uVoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import itertools\n",
        "\n",
        "def split_and_zero_padding(df, max_seq_length):  # 调整tokens长度\n",
        "\n",
        "    # 训练集矩阵转换成字典\n",
        "    X = {'left': df['question1_n'], 'right': df['question2_n']}\n",
        "\n",
        "    # 调整到规定长度\n",
        "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
        "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "vXIGelMEvAAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "class ManDist(Layer):  # 封装成keras层的曼哈顿距离计算\n",
        "\n",
        "    # 初始化ManDist层，此时不需要任何参数输入\n",
        "    def __init__(self, **kwargs):\n",
        "        self.result = None\n",
        "        super(ManDist, self).__init__(**kwargs)\n",
        "\n",
        "    # Automatically build ManDist layer\n",
        "    def build(self, input_shape):\n",
        "        super(ManDist, self).build(input_shape)\n",
        "\n",
        "    # Calculate Manhattan distance\n",
        "    def call(self, x, **kwargs):\n",
        "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
        "        return self.result\n",
        "\n",
        "    # return result\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return K.int_shape(self.result)"
      ],
      "metadata": {
        "id": "BYdIJgzVyz1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def validateEvalDirExist(dirpath):\n",
        "  dirname = os.path.dirname(dirpath)\n",
        "  if not os.path.exists(dirname):\n",
        "    os.makedirs(dirname)"
      ],
      "metadata": {
        "id": "nLBVBz9J8kjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}